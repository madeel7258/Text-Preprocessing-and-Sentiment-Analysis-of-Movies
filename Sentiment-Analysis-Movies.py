# -*- coding: utf-8 -*-
"""adeel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wBmbRjZqBsHusSITESTjcl35FihSajKW
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, classification_report
import pickle

import pandas as pd
import seaborn as sns
import string
import nltk
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.tokenize import word_tokenize

# Download necessary resources
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Load stopwords
stop = []
file_path = '/content/drive/My Drive/SoftStream.ai/NLP Assessment 1 (Sentiment)/stopwords.txt'
st = pd.read_csv(file_path, encoding="ISO-8859-1", header=None)
stop = list(st[0])
print(stop)

# Load the dataset
file_path = '/content/drive/My Drive/SoftStream.ai/NLP Assessment 1 (Sentiment)/IMDB_Dataset.csv'
data = pd.read_csv(file_path, usecols=[0, 1], names=['review', 'sentiment'], encoding="ISO-8859-1", header=None)

data.head()

import matplotlib.pyplot as plt
import seaborn as sns

# Plot the sentiment distribution
plt.figure(figsize=(4, 3))
sns.barplot(x=data['sentiment'].value_counts().index, y=data['sentiment'].value_counts().values)

# Add labels and title
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.title('Sentiment Distribution')
plt.show()

# Preprocessing function
def review_preprocessing(input_review):
    input_review = input_review.astype(str).str.lower() # Convert to lower case
    input_review = input_review.astype(str).str.replace('[{}]'.format(string.punctuation), '') # Remove punctuation
    input_review = input_review.astype(str).str.replace("[^a-zA-Z#]", ' ') # Remove special characters
    input_review = input_review.apply(remove_numbers) # Remove numbers
    input_review = input_review.astype(str).str.strip() # Remove spaces
    return input_review

def remove_numbers(text):
    words = text.split()
    words = [word for word in words if not word.isdigit()]
    return ' '.join(words)

# Initialize the stemmer and lemmatizer
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()


# Lemmatization function
def lemmatize_sentences(text):
    words = word_tokenize(text)
    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]
    return ' '.join(lemmatized_words)

# Apply preprocessing
data['review'] = review_preprocessing(data['review'])

# Apply stemming and lemmatization without using lambda
lemmatized_reviews = []

for review in data['review']:
    lemmatized_reviews.append(lemmatize_sentences(review))

# Add results back to the dataframe
data['review_lemmatized'] = lemmatized_reviews

# Remove duplicates
data = data.drop_duplicates(keep='first').reset_index(drop=True)

# Display the first few rows of the dataframe
print(data.head())



# Apply lemmatization
data['review_processed'] = data['review'].apply(lemmatize_sentences)

# Split the data into training and testing sets
X = data['review_processed'].values
y = data['sentiment'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)

# Save the processed data to a CSV file
file_path = '/content/drive/My Drive/SoftStream.ai/NLP Assessment 1 (Sentiment)/IMDB_Dataset_Processed.csv'
data.to_csv(file_path, index=False)

# Convert a collection of raw documents to a matrix of TF-IDF features
vectorizer = TfidfVectorizer(ngram_range=(1, 3))
X_train_vector = vectorizer.fit_transform(X_train)
X_test_vector = vectorizer.transform(X_test)

from sklearn.naive_bayes import MultinomialNB

# Create and train the classifier
classifier = SVC(kernel='linear', C=1.0, degree=3, random_state=0)
classifier.fit(X_train_vector, y_train)

# ##Other classifiers can be used instead, like:


### Naive Bayes
# classifier = KNeighborsClassifier(n_neighbors=3)
# classifier.fit(X_train_vector, y_train)

###AdaBoost
# classifier = AdaBoostClassifier(n_estimators=50, learning_rate=1)
# classifier.fit(X_train_vector, y_train)

###RandomForest
# classifier = RandomForestClassifier(n_estimators=100, random_state=0)
# classifier.fit(X_train_vector, y_train)

###naive_bayes
# classifier = naive_bayes.MultinomialNB()
# classifier.fit(X_train_vector, y_train)

###LogisticRegression
# classifier = LogisticRegression(random_state=0, solver='liblinear', multi_class='ovr')
# classifier.fit(X_train_vector, y_train)

# Predict the labels for the test set
y_pred = classifier.predict(X_test_vector)

# Create a confusion matrix
ConfusionMatrix = confusion_matrix(y_test, y_pred)
print(ConfusionMatrix)

# Print the classification report
print(classification_report(y_test, y_pred))

# Calculate and print accuracy
Accuracy = format(classifier.score(X_test_vector, y_test) * 100, '.2f') + ' %'
print(Accuracy)

# Save accuracy to a file
with open('AccuracyPercentage', 'wb') as file:
    pickle.dump(Accuracy, file)

# Visualize the confusion matrix
labels = ['Positive', 'Negative']
fig = plt.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(ConfusionMatrix)
plt.title('Confusion matrix of the classifier \n')
fig.colorbar(cax)
ax.set_xticklabels([''] + labels)
ax.set_yticklabels([''] + labels)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

